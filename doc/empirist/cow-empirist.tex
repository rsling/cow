\documentclass[11pt]{article}
\usepackage{acl2016}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}

\newcommand{\Lf}{
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}

\title{The COW14 Tool Chain for German}

\author{Roland Schäfer \\
  Linguistic Web Characterization (DFG) \\
  Freie Universität Berlin \\
  {\tt roland.schaefer@fu-berlin.de} \\\And
  Felix Bildhauer \\
  Grammar Department \\
  Institut für Deutsche Sprache Mannheim \\
  {\tt bildhauer@ids-mannheim.de} \\}

\date{}

\begin{document}

\maketitle

\begin{abstract}
  In this paper, we describe the COW14 tokenization and part-of-speech tagging pipeline for German, with which we participated in the EmpiriST 2015 shared task.
  We briefly discuss the original design goals for the tool chain and the minimal changes we made for the shared task.
  It should be noticed that we did not expect our system to perform competitively, especially not on CMC data and in the POS tagging track, and that we consequently viewed our system as an improved baseline system.
  This considered, the results, which we also report, are exceptionally good.
\end{abstract}

\section{Original Design Goals}
\label{sec:originaldesigngoals}

This section describes the original design goals of our tool chain.%
\footnote{The tools have been freely available since 2014.
  They are distributed under a permissive 2-clause BSD license on GitHub (\url{https://github.com/rsling/cow}).}
Since we participated with a production system only minimally adapted for the EmpiriST 2015 shared task, it is important to keep in mind the intended use and users of the system when evaluating the performance of and the errors made by the system.

The system with which we participated is the production system implemented for the construction of the COW and CommonCOW corpora that are described in \newcite{SchaeferBildhauer2012a}, \newcite{Schaefer2015b}, \newcite{Schaefer2016a}, but also (in comparison to other initiatives) in \newcite{BiemannEa2013} and (implicitly) in textbook form in \newcite{SchaeferBildhauer2013}.%
\footnote{\url{http://corporafromtheweb.org}}
Instead of developing custom tools for linguistic annotation---as we did for the non-linguistic preprocessing in the form of the \textit{texrex} web page processor \cite{SchaeferBildhauer2012a,Schaefer2015b,Schaefer2016c}---we wrapped available tools and models (tokenizers, POS taggers and lemmatizers, morphological analyzers, named-entity recognizers, and dependency parsers).

Since the COW tool chain is a production system, we consider it vital to understand who our primary users are, and what the resulting design goals were.
We have a background in theoretical linguistics, especially morpho-syntax and semantics.
For empirical work in these fields, corpora have become a major source of data, and when it comes to rare phenomena, large web corpora containing a fair share of non-standard language are sometimes even the only available source of data.
However, our intended primary users (including ourselves) are not interested in the specifics of computer-mediated communication but rather in a broad view of variation and alternations occurring in present-day language.
This means that linguistic annotations like POS tags can only be utilized by this type of user if they are near-perfect, and corpus queries consequently have a near-perfect recall.

This is reflected in the behavior of our users.
From log analyses of the queries made by our users at \url{https://webcorpora.org}, we know that (as of 15 May 2016) POS tag specifications are used in only 13.7\%, lemma information in 14.13\%, and simple token specifications in 89.2\% of all queries to our DECOW-corpus (German; figures are about the same for COW corpora in other languages).
In other words, most linguists who use our web corpora do not use POS tags---let alone other types of annotation---either because they are unaware of what such annotations can do for them, or because they cannot risk publishing corpus studies where material was not taken into account because the tagger made crucial errors, and queries specifying POS annotations did not return all the relevant targets.
While low precision for corpus queries just means that corpus linguists have to filter their concordances by hand more thoroughly, below 100\% recall runs the danger of invalidating corpus studies.

As an example, the named entity annotation in DECOW14 was added because some productive DECOW users work on the morpho-syntax of German person names.
After the real-life accuracy of the Stanford Named Entity Recognizer with the models from \newcite{FaruquiPado2010} was evaluated as unacceptable \cite{Helmers2013}, carefully designed heuristics and lists of names created by hand were used instead of the automatically generated annotation \cite{Ackermann2016}.
The entire automatic annotation was essentially useless, at least for the intended use.

%That said, improving POS tagging has never been our primary focus, simply because we are aware that near-perfect results cannot be achieved anyway.
That said, improving POS tagging has never been our primary focus, simply because we doubt that near-perfect results can be achieved for the mixture
of text types typically found in crawled web corpora.
Since corpus query engines always use tokens as their basic unit for indexing, near-perfect tokenization is a de facto-requirement, however.
This is why we invested considerable effort into tokenization but not into POS tagging.
We describe the COW14 tool chain for German in Section~\ref{sec:implementation}, report the results on the EmpiriST 2015 data sets in Section~\ref{sec:results} before summarizing the paper in Section~\ref{sec:summaryoutlook}.

\section{Implementation}
\label{sec:implementation}

Before we go into the technical details of the COW14 tool chain, we would like to mention our main woe (as corpus creators), namely the fact that, even in 2016, most NLP tools generally do not come with the ability to process text contained in XML files, which---in the simplest and totally sufficient case---means skipping over anything in $<>$ and treating the five canonical XML entities as their literal counterparts.
The problem exists across the board with Ucto and TreeTagger discussed below, but also with all other tools we use, such as FreeLing \cite{PadroStanislavsky2012}, the Stanford Named Entity Recognizer \cite{FaruquiPado2010}, mate-tools \cite{BohnetNivre2012}, and Marmot \cite{MuellerEa2013}.
Such problems are not usually tackled in shared tasks where accuracy is, of course, the only metric of interest and usability in large production systems is less relevant.
That said, we now proceed to the technical details.

\subsection{Tokenization}
\label{sec:tokenizer}

The COW14 tool chain is merely a series of script wrappers around existing tools.
Since we use our university's SLURM-based high-performance cluster for gigatoken corpus creation and SLURM is best controlled using Bash scripts, all tools are wrapped in Bash scripts.%
\footnote{\url{http://slurm.schedmd.com/}}

We currently use the rule-based Ucto tokenizer for tokenization and heuristic sentence splitting \cite{ucto}.
It is wrapped in a script that also performs some pre- and post-processing.
Unfortunately, much of this processing goes into keeping Ucto from separating material that should not be separated.
In general, we find it highly difficult to write clean rule sets for Ucto without triggering completely unpredictable side-effects.%
\footnote{Most side effects, we think, are due to the fact that Ucto compiles the rules into complex regular expressions for the ICU library.
For example, we observed cases where the scopes of matched groups and replacement operators were obviously mangled, leading to unsolicited replacements.}
Also, Ucto's added functionality of being able to discern different types of tokens (numbers, dates, etc.), while an interesting (yet little documented) feature, makes writing rule sets complicated and unpredictable.
As a consequence, we are currently designing our own rule-based tokenizer and are planning to move our entire rule set from Ucto to the new system.

Pre-processing includes:

\begin{itemize}\Lf
  \item heuristically separating run-together sentences (using a custom tool included in the \textit{texrex} suite; not used for the EmpiriST data due to problems with the file format)
  \item substituting any whitespace withing XML-tags (e.\,g., between attribute-value pairs) because Ucto cannot deal with XML tags 
  \item converting XML entities to literals because Ucto cannot deal with XML entities
  \item marking certain kinds of strings in a way that they are not broken up by Ucto, for example double names written as \textit{Kay-M.}, file names, DOIs, ISBNs, content-type declarations, dates and numbers with periods (otherwise often detected as sentence ends)
  \item pre-processing quotes to make sure they are always treated as separate tokens
  \item pre-processing obfuscated email addresses with \textit{[at]} instead of @
\end{itemize}

In the Ucto rule set, we have rules (some copied from the generic Ucto profile for German) which recognize, for example,

\begin{itemize}\Lf
  \item email addresses and URLs
  \item dates and numbers
  \item various special abbreviation-like tokens such as \textit{H\&M} and \textit{C++}
  \item number-letter combinations that should not be split such as \textit{90-fach} (\textit{90-fold})
  \item over 250 custom abbreviations (plus some regexes which detect abbreviations heuristically)
\end{itemize}

The post-processing mainly deals with restoring material that was protected from separation in pre-processing.

Virtually all of this was there before the EmpiriST 2015 shared task.
We merely changed the way some tokens were treated because we had different design goals.
For example, we previously kept single-word strings with asterisks like \textit{*freu*} as one token but split up similar multi-word strings such \textit{*total\textvisiblespace freu*} as \textit{*\textvisiblespace total\textvisiblespace freu\textvisiblespace *} (4 tokens).
However, we did not even attempt to achieve full compatibility with the EmpiriST guidelines.

\subsection{POS Tagging}
\label{sec:postagger}

For POS tagging, we use the TreeTagger \cite{Schmid1994b,Schmid1995} with the standard models.
The Bash wrapper's main function is to set up a correct piping between different TreeTagger instances for tagging and chunking with other scripts in between.
The only improvements we implemented in the wrapper concern regular expression-based recognition of smileys and other emoticons as well as some special tokens inserted by our web page processing system \textit{texrex}.
This applies \textit{after} TreeTagger, overriding some POS tags assigned by TreeTagger.
It cannot be implemented by pre-tagging, simply because introducing new POS tags for smileys etc.\ would require an extended tag set and consequently a re-trained tagger model.

Other than that, we only improved lemmatization and POS tagging by amending the tagger lexicon.
We sorted the frequency list of tokens lemmatized as \textit{unknown} in a large subset of DECOW14A and manually created a 3,800 entries long lexicon addition for TreeTagger with POS and lemmas in order to take care of the most frequent unknown words.%
\footnote{We stopped at the point where the list contained less than one fixable \textit{unknown} token in a window of thirty tokens.
The remaining \textit{unknowns} are productively formed compounds and noise.}
While this necessarily also improves the quality of the POS tagging in our full corpus, it most likely did not improve the results in the EmpiriST 2015 shared task, simply because the sample is so small that the added words do not occur in it with high enough frequencies.

\section{Results}
\label{sec:results}

\begin{table}[!htb]
  \centering
  \begin{tabular}{|l|rrr|}
    \hline
    \textbf{Dataset} & \textbf{Prec} & \textbf{Rec} & $\mathrm{\bf f_1}$ \\
    \hline
    Web & 99.84 & 99.71 & 99.77 \\
    CMC & 98.31 & 98.07 & 98.18 \\
    \hline
  \end{tabular}
  \caption{COW14 results on EmpiriST 2015 tokenization data sets}
  \label{tab:resultstok}
\end{table}

Table~\ref{tab:resultstok} summarizes the COW14 results on the EmpiriST 2015 web and CMC tokenization gold standard data sets according to the official evaluation script.
The near-perfect performance on web data is not surprising because this is the type of data for which we optimized our tool chain.
The remaining errors are analyzed in Table~\ref{tab:analyzeweb}.
Except for the serious error labeled \textit{concatenation} and the split-up emoticon, all the other errors are irrelevant for our primary target users, as explained in Section~\ref{sec:originaldesigngoals}.%
\footnote{We were surprised to see the emoticon being incorrectly tokenized because there \textit{is} a rule in our Ucto rule set which should detect emoticons like :)
We suspect that this is yet another Ucto side-effect, maybe introduced by the tweaks we made in order to conform to the EmpiriST format.}
In other words, for our purposes, tokenization is a solved problem.

\begin{table*}[!htb]
  \centering
  \scalebox{0.9}{
  \begin{tabular}{|r|lll|}
    \hline
    \textbf{Count} & \textbf{Type} & \textbf{Example Gold} & \textbf{Example COW14} \\
    \hline
     16 & hyphenization & Herford\textvisiblespace--\textvisiblespace Altenbeken & Herford--Altenbeken \\
     5  & brackets & Geschichte\textvisiblespace[\textvisiblespace Bearbeiten\textvisiblespace] & Geschichte[Bearbeiten] \\
     4  & punctuation clusters & :!: & :\textvisiblespace!\textvisiblespace: \\
     2  & mixed alphanumeric tokens & R1 & R\textvisiblespace 1\\
     2  & years with periods & 1814\textvisiblespace. & 1814.  \\
     2  & dates & 2015\textvisiblespace /11\textvisiblespace /22 & 2025\textvisiblespace /\textvisiblespace 11\textvisiblespace /\textvisiblespace 22 \\
     1  & truncated compounds & Viren-\textvisiblespace/\textvisiblespace Spywarescanner & Viren\textvisiblespace-\textvisiblespace/\textvisiblespace Spywarescanner \\
     1  & concatenation & eBook\textvisiblespace Das & eBookDas \\
     1  & emoticon & :) & :\textvisiblespace) \\
    \hline
  \end{tabular}
  }
  \caption{Breakdown of tokenizer errors by types for the web data}
  \label{tab:analyzeweb}
\end{table*}

\begin{table}[!htb]
  \centering
  \begin{tabular}{|l|rr|}
    \hline
                     & \textbf{Accuracy} & \textbf{Accuracy} \\
    \textbf{Dataset} & \textbf{STTS} & \textbf{Extended} \\
    \hline
    Web & 92.96 & 91.82 \\ 
    CMC & 81.49 & 77.89 \\
    \hline
  \end{tabular}
  \caption{COW14 results on EmpiriST 2015 tagging data sets}
  \label{tab:resultstag}
\end{table}

We do not discuss the tokenization errors for the CMC data set in detail because we did not optimize out tokenizer for such data.
There are some errors like the \textit{concatenation} error from Table~\ref{tab:analyzeweb}, many emoticons which were broken up, and incorrect handling of run-together sentences.
Since we normally fix run-together sentences before tokenization, the error rate can be expected to be lower on data that has run through our complete pipeline.

For completeness, we provide the tagging results are given in Table~\ref{tab:resultstag}.
We do not discuss the errors in detail because our contribution was non-competitive.
Also, we did not implement the extensions to STTS introduced for EmpiriST \cite{BeisswengerEa2016}, and the evaluation against the extended tag set is virtually meaningless.%
\footnote{
We would like to point out that in a post-hoc evaluation on 18,993 tokens from DECOW14A that we did ourselves, we found that TreeTagger achieves an accuracy of 96.93\% (standard STTS).
However, this evaluation was confined to tokens occurring within regions labelled as sentences by our tool chain, and we apply a large number of heuristics in order to make sure that only very clean regions are labelled as sentences.
Users are then advised to rely on annotation only in regions labeled as sentences.
Thus, we provide the best possible quality of annotation for the subset of the corpus where high quality can be reached, and we leave all other (potentially noisy) material in the corpus for completeness (see also Section~\ref{sec:originaldesigngoals}).
}

\section{Summary and Outlook}
\label{sec:summaryoutlook}

For our purposes, word-level tokenization of web texts is a solved task.
Interestingly, rule-based approaches such as ours apparently achieve highly competitive accuracy.
In our own work, we will therefore rather focus on sentence segmentation, which appears to us to be less of a solved task.

As for POS tagging, the situation is far less satisfying, especially considering that even the EmpiriST 2015 winner in the web tagging track (UdS-distributional) achieved only 94.62\% accuracy (plain STTS) according to the preliminary results released by the EmpiriST organizers.
This is no more than 1.66\% better compared to our baseline system.
From a linguistic point of view, we have doubts that the kind of creative and non-standard language found on the web (and even more so in CMC) can be dealt with by extending tag sets and improving guidelines.
Standardized categorization of non-standard data is---at least partly---a contradiction in terms.

Importantly, we suggest that discussions of changes or extensions to tag sets should involve the largest possible group of users because we see linguistic annotation purely as a service that we provide to make our corpora more usable for our users.
Most of our users are not specifically interested in CMC, but rather in large corpora which contain a certain amount of non-standard grammar.
Therefore, we are planning to conduct a survey among the users of our corpora and ask them how they use the linguistic annotation provided by us, and in which directions they would like to see them improve or change.

\section*{Acknowledgments}

Roland Schäfer's contribution to this paper was funded by the German Research Council (Deutsche Forschungsgemeinschaft, DFG) through the personal grant SCHA1916/1-1 \textit{Linguistic Web Characterization}.

We would like to thank the high performance computing (HPC) unit of ZEDAT data center at Freie Universität Berlin for the provided computing time.

\bibliography{rs,cow}
\bibliographystyle{acl2016}

%\appendix

\end{document}
