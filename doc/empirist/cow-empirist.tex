\documentclass[11pt]{article}
\usepackage{acl2016}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage[utf8]{inputenc}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}

\newcommand{\Lf}{
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}

\title{COW14 Tokenization, Tagging (and Lemmatization)}

\author{Roland Schäfer \\
  Linguistic Web Characterization (DFG) \\
  Freie Universität Berlin \\
  {\tt roland.schaefer@fu-berlin.de} \\\And
  Felix Bildhauer \\
  Grammar Department \\
  Institut für Deutsche Sprache Mannheim \\
  {\tt bildhauer@ids-mannheim.de} \\}

\date{}

\begin{document}

\maketitle

\begin{abstract}
  In this paper, we describe the COW tokenization and part-of-speech tagging pipeline with which we participated in the EmpiriST 2015 shared task.
  We briely discuss the original design goals for the tool chain and the minimal changes we made for the shared task.
  It should be noticed that we did not expect our system to perform competitively, especially not on CMC data and in the POS tagging track, and that we consequently viewed our system as an improved baseline system.
\end{abstract}

\section{Original Design Goals}
\label{sec:originaldesigngoals}

This section describes the original design goals of our toolchain.%
\footnote{The tools have been freely available since 2014.
  They are distributed under a permissive 2-clasue BSD license on GitHub (\url{https://github.com/rsling/cow}).}
Since we participtated with a production system only minimally adapted for the EmpiriST 2015 shared task, it is important to keep in mind the intended use and users of the system when evaluating the performance of and the errors made by the system.

The system with which we participated is the production system implemented for the construction of the COW and CommonCOW corpora that are described in \newcite{SchaeferBildhauer2012a}, \newcite{Schaefer2015b}, \newcite{Schaefer2016a}, but also (in comparison to other initiatives) in \newcite{BiemannEa2013} and (implicitly) in textbook form in \newcite{SchaeferBildhauer2013}.%
\footnote{\url{http://corporafromtheweb.org}}
Instead of developing custom tools for linguistic annotation---as we did for the non-linguistic preprocessing in the form of the \textit{texrex} web page processor \cite{SchaeferBildhauer2012a,Schaefer2015b,Schaefer2016c}---we wrapped available tools and models (tokenizers, POS taggers and lemmatizers, morphological analyzers, named-entity recognizers, and dependency parsers).
Section~\ref{sec:implementation} describes technical details, including some improvements we achieved through custom pre- and postprocessing.

However, since the COW toolchain is a production system, it is vital to understand who our primary customers are.
We have a background in theoretical linguistics, especially morpho-syntax and semantics.
For empirical work in these fields, corpora have become a major source of data.

From log analyses of the queries made by our users at \url{https://webcorpora.org}, we know that POS tag specifications are used in only 14.8\% (DECOW-only 13.7\%), lemma information in 13.39\% (DECOW-only 14.13\%), and simple token specifications in 89.2\% (DECOW-only 89.2\%) of all queries.
This illustrates that most linguists who use (our) corpora do not profit from high quality POS tagging---let alone other types of annotation---either because they are unaware of what such annotations can do for them, or because they cannot risk publishing corpus studies where material was not taken into account because the tagger made crucial errors, and queries did not return all the relevant targets.
In other words, while low precision for corpus queries just means that corpus linguists have to filter their concordances by hand more thoroughly, below 100\% recall runs the danger of invalidating corpus studies.

As an example, the named entity annotation in DECOW14 was added because some productive DECOW users work on the morphosyntax of German person names.
After the real-life accuracy of the Stanford Named Entity Recognizer with the models from \newcite{FaruquiPado2010} was evaluated as inacceptable \cite{Helmers2013}, carefully designed heuristics and lists of names created by hand were used instead of the automatically generated annotation \cite{Ackermann2016}.
The entire automatic annotation was essentially useless, at least for the intended use.

Furthermore, users usually do not ask for more fine-grained POS annotations, but for more coarse-grained ones.

\section{Implementation}
\label{sec:implementation}

Since we use our university's SLURM-based high-performance cluster and SLURM is best controlled from Bash scripts, all tools are wrapped in Bash scripts.

As corpus creators, we would like to mention our main woe is that, even in 2016, NLP tools generally do not come with the ability to process text in XML, which---in the simplest and totally sufficient case---means skipping over anything in $<>$ and treating the five canonical XML entities as their literal counterparts.
This is clearly the point where we lose most time and resources.
The problem exists across the board with Ucto and TreeTagger discussed below, but also with all other tools we use, such as FreeLing \cite{CarrerasEa2004}, the Stanford Named Entity Recognizer \cite{FaruquiPado2010}, mate-tools \cite{BohnetNivre2012}, and Marmot \cite{MuellerEa2013}.
Such problems are not usually tackled in shared tasks where accuracy is, of course, the only metric of interest, and usability in large production systems is less relevant.%
\footnote{Please note that in many production systems, using UIMA wrappers is not feasible technically.
The same goes for Python NLTK.}
That said, we now proceed to the technical details.

\subsection{Tokenizer}
\label{sec:tokenizer}

We currently use the rule-based Ucto tokenizer for tokenization and heuristic sentence splitting \cite{ucto}.
It is wrapped in a Bash script which also performs some pre- and post-processing.
Unfortunately, much of this processing goes into keeping Ucto from separating material that should not be separated.
In general, we find it highly difficult to write clean rule sets for Ucto without triggering completely unpredictable side-effects.%
\footnote{Most side effects, we think, are due to the fact that Ucto compiles the rules into complex regular expressions for the ICU library.
For example, we observed cases where the scopes of matched groups and replacement operators were obviously mangled, leading to unsolicited replacements.}
Also, Ucto's added functionality of being able to discern different types of tokens (numbers, dates, etc.), while an interesting (yet little documented) feature, makes writing rule sets complicated and unpredictable.
As a consequence, we are currently designing our own rule-based tokenizer and are planning to move our entire rule set from Ucto to the new system.

Pre-processing includes:

\begin{itemize}\Lf
  \item converting XML entities to literals because Ucto cannot deal with XML entities
  \item marking certain kinds of strings in a way that they are not broken up by Ucto, for example double names written as \textit{Kay-M.}, file names, DOIs, ISBNs, content-type declarations, dates and numbers with periods (otherwise often detected as sentence ends)
  \item pre-processing quotes to make sure they are always treated as separate tokens
  \item pre-processing obfuscated email addresses with \textit{[at]} instead of @.
\end{itemize}

In the Ucto rule set, we have rules (some copied from the generic Ucto profile for German) which recognize, for example,

\begin{itemize}\Lf
  \item email addresses and URLs
  \item dates and numbers
  \item various special abbreviation-like tokens such as \textit{H\&M} and \textit{C++}
  \item number-letter combinations that should not be split such as \textit{90-fach} (\textit{90-fold})
  \item over 250 custom abbreviations (plus some regexes which detect abbreviations heuristically)
\end{itemize}

The post-processing mainly deals with restoring material that was protected from separation in pre-processing.

Virtually all of this was there before the EmpiriST 2015 shared task.
We merely changed the way some tokens are treated.
For example, we made the decision to keep single-word strings with asterisks like \textit{*freu*} as one token but to split up similar multi-word strings such \textit{*total freu*} as \textit{* total freu *} (4 tokens).
Also, we previously tokenized dates as one token (\textit{2016-05-01}), simply because our users are not usually interested in dates at all, and the best we could do is keep the token count low for such strings.

\subsection{POS Tagger}
\label{sec:postagger}

For POS tagging, we use the TreeTagger \cite{Schmid1994b} with the standard models.
The Bash wrapper's main function is to set up a correct piping between different TreeTagger instances for tagging and chunking with other scripts in between.
The only improvements we implemented in the wrapper concern regular expression-based recognition of smileys and other emoticons as well as some \textit{blank}-tokens inserted by our web page processing system \textit{texrex}.
This applies after TreeTagger.
It cannot be implemented by pre-tagging, simply because introducing new POS tags for smileys would require an extended tag set and consequently a re-trained tagger model.

Other than that we only improved lemmatization and POS tagging by amending the tagger lexicon.
We sorted the frequency list of tokens lemmatized as \textit{unknown} in a large subset of DECOW14A and manually created a 3,800 entries long lexicon addition for TreeTagger with POS and lemmas in order to take care of the most frequent unknown words.%
\footnote{We stopped at the point where the list contained less than one fixable \textit{unknown} token in a window of thirty tokens.
The remaning unknowns are productively formed compounds and noise.}
While this necessarily also improves the quality of the POS tagging in our full corpus, it most likely did not improve the results in the EmpiriST 2015 shared task, simply because the sample is so small that the added words do not occur in it with high enough frequency.

\section{Discussion of the Results}
\label{sec:discussionresults}

\section{Summary and Outlook}
\label{sec:problemsoutlook}

As we pointed out, designers of production systems are faced with problems that are not within the scope of shared tasks, such as compatibility of tools with input formats.
However, we suggest that discussions of changes or extensions to tag sets---a problem well within the scope of efforts like EmpiriST 2015---should involve the largest possible group of users because we see linguistic annotation purely as a service we provide to make our corpora more usable for our users.
Therefore, we are planning to conduct a survey aong the users of our corpora and ask them how they use the linguistic annotation provided by us, and in which directions they would like to see them improve 

\section*{Acknowledgments}

Roland Schäfer's contribution to this paper was funded by the German Research Council (Deutsche Forschungsgemeinschaft, DFG) through the personal grant SCHA1916/1-1 \textit{Linguistic Web Characterization}.

We would like to thank the high performance computing (HPC) unit of ZEDAT data center at Freie Universität Berlin for the provided computing time.

\bibliography{rs,cow}
\bibliographystyle{acl2016}

%\appendix

\end{document}
